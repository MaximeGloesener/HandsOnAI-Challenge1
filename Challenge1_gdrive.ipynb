{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximeGloesener/HandsOnAI-Challenge1/blob/master/Challenge1_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK6fZUXolwPs"
      },
      "source": [
        "# **1. Hardware Informations (GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjh8Rulquq"
      },
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash"
      ],
      "metadata": {
        "id": "uqK0POsY1lmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrtS3fRhb5x6"
      },
      "source": [
        "# **2. Importation of librairies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS3XuLGyb5x_"
      },
      "source": [
        "from IPython.display import Image, HTML, display\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np \n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input #224*224\n",
        "from keras.applications.xception import Xception, preprocess_input, decode_predictions #299*299\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions #224*224\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Flatten, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import math\n",
        "import argparse\n",
        "import matplotlib\n",
        "import imghdr\n",
        "import pickle as pkl\n",
        "import datetime\n",
        "from cycler import cycler\n",
        "from PIL import Image, ImageEnhance\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import imagehash\n",
        "print(\"Tensorflow version: \"+tf.__version__)\n",
        "print(\"Keras version: \" + tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6HqNeyYKraU"
      },
      "source": [
        "#**3. Download of training datasets \"FIRE_DATABASE_X\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CwAmMbqmGhW"
      },
      "source": [
        "bases_path_after=\"bases\"\n",
        "test=\"test_data\"\n",
        "if os.path.exists(bases_path_after) == False:\n",
        "    os.makedirs(bases_path_after)\n",
        "if not os.path.exists(test):\n",
        "  os.makedirs(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Données de test\n",
        "!rm -rf sample_data\n",
        "!wget --no-check-certificate http://195.154.53.219/downloads/test.tar\n",
        "! tar xf test.tar -C 'test_data' --one-top-level\n",
        "! rm test.tar"
      ],
      "metadata": {
        "id": "Us_JlUVUXamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important de tester les doublons en utilisant un hash cryptographique qui comparer les images pixels par pixels. Avec un hash robuste, on trouve des faux doublons. Le hash robuste permet de détecter les doublons si resize/légère modifiction mais ce n'est pas le cas ici dans les datasets. "
      ],
      "metadata": {
        "id": "prGdb9QW88gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(file_name):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée une path d'image et qui return RGB (utile pour plot)\n",
        "  \"\"\"\n",
        "  img = cv2.imread(file_name, 3)\n",
        "  b,g,r = cv2.split(img)\n",
        "  rgb_image = cv2.merge([r,g,b])\n",
        "  return rgb_image\n",
        "\n",
        "def plot(images, noms):\n",
        "  f, axarr = plt.subplots(1,len(images))\n",
        "  for i in range(len(images)):\n",
        "    axarr[i].imshow(images[i])\n",
        "    axarr[i].title.set_text(noms[i])\n"
      ],
      "metadata": {
        "id": "-T0WHM1aQShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des données\n",
        "# On sait que dans les datasets, il y a parfois plusieurs fois la même image\n",
        "# But : analyser chaque dataset et trouver le nombre d'images en doublons\n",
        "def analyse_dataset(folder_name, affichage = False):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée le directory d'un dataset et qui va chercher les images qui sont présentes plusieurs fois pour ce même dataset\n",
        "  Affichage = True si on veut plot les images qui sont en doubles et leur nom\n",
        "  Return: - le nombre de doublons dans un dataset\n",
        "          - le pourcentage de doublons\n",
        "  \"\"\"\n",
        "  img_hashes = dict()\n",
        "  total = 0\n",
        "  doublons = 0\n",
        "\n",
        "  for dir in os.listdir(folder_name):\n",
        "    for image in os.listdir(os.path.join(folder_name, dir)):\n",
        "      total += 1\n",
        "      image = os.path.join(os.getcwd(), folder_name, dir, image)\n",
        "      hash = imagehash.dhash(Image.open(image))\n",
        "      if hash in img_hashes:\n",
        "        doublons += 1\n",
        "        #print(f'{image} doublons de {img_hashes[hash]}')\n",
        "        if affichage:\n",
        "          i = read_image(image) \n",
        "          x = read_image(img_hashes[hash])\n",
        "          plot([x,i],[image.split(\"/\")[-1], img_hashes[hash].split(\"/\")[-1]])\n",
        "      else:\n",
        "        img_hashes[hash] = image\n",
        "\n",
        "  return doublons, doublons/total*100\n",
        "\"\"\"\n",
        "d1, p1 = analyse_dataset(\"/content/bases/FIRE_DATABASE_1/\")\n",
        "d2, p2 = analyse_dataset(\"/content/bases/FIRE_DATABASE_2/\")\n",
        "d3, p3 = analyse_dataset(\"/content/bases/FIRE_DATABASE_3/\", affichage = True)\n",
        "print('DATASET 1 ')\n",
        "print(f'Il y a {d1} doublons dans le dataset = {p1}% des données')\n",
        "print('DATASET 2 ')\n",
        "print(f'Il y a {d2} doublons dans le dataset = {p2}% des données')\n",
        "print('DATASET 3 ')\n",
        "print(f'Il y a {d3} doublons dans le dataset = {p3}% des données')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tIpQq5lygtsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un seul dataset à partir des 3 en ne prenant en compte que des images uniques (supprimer tous les doublons)\n",
        "def make_dataset(base_directory):\n",
        "  \"\"\"\n",
        "  Fonction qui va concaténer les 3 datasets de départ et créer un seul dataset sans doublons\n",
        "  Return les hashs des images déjà présentes dans le dataset -> utile lors de la phase data augmentation pour \n",
        "  ne pas rajouter des images qui sont déjà présentes dans la jeu de données \n",
        "  \"\"\"\n",
        "  !rm -rf all_data\n",
        "  directory = 'all_data'\n",
        "  directory_path = os.path.join(os.getcwd(), directory)\n",
        "  # créer un nouveau directory all_data s'il n'existe pas déjà\n",
        "  if not os.path.exists(directory_path):\n",
        "    os.mkdir(directory_path)\n",
        "    os.mkdir(os.path.join(directory_path, \"fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"no_fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"start_fire\"))\n",
        "\n",
        "  images_hash = set()\n",
        "  for dir in os.listdir(base_directory):\n",
        "    for dir2 in os.listdir(os.path.join(base_directory,dir)):\n",
        "      for img in os.listdir(os.path.join(base_directory,dir,dir2)):\n",
        "        path = os.path.join(os.getcwd(), base_directory, dir, dir2, img)\n",
        "        hash = imagehash.dhash(Image.open(path))\n",
        "        if hash not in images_hash:\n",
        "          images_hash.add(hash)\n",
        "          cv2.imwrite(os.path.join(directory_path, dir2, img), cv2.imread(path))\n",
        "  return images_hash \n",
        "hashes = make_dataset(\"bases\")"
      ],
      "metadata": {
        "id": "e0FpgawA8NHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#%cp -av \"/content/all_data/\" \"/content/gdrive/MyDrive/Challenge1/\""
      ],
      "metadata": {
        "id": "HXAc626acFRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"gdrive/MyDrive/Challenge1/all_data/fire\")))\n",
        "print(len(os.listdir(\"gdrive/MyDrive/Challenge1/all_data/start_fire\")))\n",
        "print(len(os.listdir(\"gdrive/MyDrive/Challenge1/all_data/no_fire\")))"
      ],
      "metadata": {
        "id": "L_vXyxSxo4PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Analyse des images dans un directory\n",
        "directory = '/content/gdrive/MyDrive/Challenge1/all_data/start_fire'\n",
        "\n",
        "for index, img in enumerate(os.listdir(directory)):\n",
        "  img = os.path.join(os.getcwd(), directory, img)\n",
        "  fig = plt.figure()\n",
        "  image = read_image(img)\n",
        "  plt.imshow(image)\n",
        "  plt.title(img)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-hc06Ha0bfZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_-BpaZnLDmn"
      },
      "source": [
        "#**4. Cretate the labels file \"classes.txt\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70eR5xy2nNZP"
      },
      "source": [
        "!printf '%s\\n' 'fire' 'no_fire' 'start_fire'> classes.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyXXoZphLN1-"
      },
      "source": [
        "#**5. Training parameters and selectioon of Pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix random seed \n",
        "tf.keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "3cIzhz5iNn7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwKf5hQLnUPi"
      },
      "source": [
        "nb_classes = 3\n",
        "nbr_batch_size=8 #@param [1,2,4,8,16,32,64,128] {type:\"raw\"}\n",
        "dataset_path = \"gdrive/MyDrive/Challenge1\"\n",
        "input_dim=224 #@param [224,299] {type:\"raw\"}  \n",
        "dataset_name='all_data' #@param [\"all_data\"]\n",
        "\n",
        "dataset_path = os.path.join(dataset_path,dataset_name)\n",
        "classes_path = \"classes.txt\"\n",
        "csv_path = 'result.csv'\n",
        "epochs = 30 #@ param {type:\"slider\", min:5, max:100, step:5}\n",
        "\n",
        "result_path='results/'\n",
        "log_path='logs'\n",
        "\n",
        "classifier = \"Xception\" #@param [\"ResNet50\",\"VGG19\",\"Xception\",\"MobileNet\",\"DenseNet169\"] {type:\"string\"}\n",
        "result_path = 'results/'+classifier\n",
        "log={\n",
        "    'epochs':epochs,\n",
        "    'batch_size':nbr_batch_size,\n",
        "    'val_loss':-1,\n",
        "    'val_acc':-1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_path)"
      ],
      "metadata": {
        "id": "94acb9Z0lc3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SBzh646b5yz"
      },
      "source": [
        "# **6. Get the number of classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ55HVhkb5y3"
      },
      "source": [
        "# Get the class names\n",
        "with open(classes_path, 'r') as f:\n",
        "    classes = f.readlines()\n",
        "    classes = list(map(lambda x: x.strip(), classes))\n",
        "num_classes = len(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_classes)"
      ],
      "metadata": {
        "id": "1SqSdQUDkL-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTyWV0pQb5zD"
      },
      "source": [
        "# **8. Selection and configuration of the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoWLjGra8Ls9"
      },
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\tdataset_path,                     # Path of the dataset\n",
        "\tvalidation_split=0.2,             # Data division : validation (20%), train (80%)\n",
        "\tsubset=\"training\",                # Selection of training data\n",
        "\tseed=42,                          # Initialization of random generator (for permutations)\n",
        "\timage_size=(224,224),    # Input size of images\n",
        "\tbatch_size=nbr_batch_size,        # Batch_size\n",
        "  label_mode=\"categorical\"     # Conversion to One-Hot format\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji2AfPd88PC-"
      },
      "source": [
        "#**9. Selection and configuration of the validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oruux4rRyDv"
      },
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\tdataset_path,                     # Path of the dataset\n",
        "\tvalidation_split=0.2,             # Data division : validation (20%), train (80%)\n",
        "\tsubset=\"validation\",                # Selection of validation data\n",
        "\tseed=42,                          # Initialization of random generator (for permutations)\n",
        "\timage_size=(224,224),    # Input size of images\n",
        "\tbatch_size=nbr_batch_size,        # Batch_size\n",
        "  label_mode=\"categorical\"     # Conversion to One-Hot format\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi8oBrIJWa5s"
      },
      "source": [
        "# **10. Download the pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WCBo9Oyb5zc"
      },
      "source": [
        "base_model = Xception(include_top = False, weights ='imagenet',input_shape = (input_dim,input_dim,3))\n",
        "model = base_model.output\n",
        "model = Flatten()(model)\n",
        "model = Dense(128,activation='relu')(model)\n",
        "model = Dropout(0.8)(model)\n",
        "model = Dense(64,activation = 'relu')(model)\n",
        "model = Dropout(0.4)(model)\n",
        "predictions = Dense(num_classes, activation = 'softmax')(model)\n",
        "model = Model(inputs=base_model.inputs, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6CWnXEPb5zX"
      },
      "source": [
        "# **13. Model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPGAZ2Vab5zo"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "# pour permettre le ré-entrainement des couches\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# recompiler le modèle\n",
        "opt = keras.optimizers.SGD(learning_rate=0.0001,decay=1e-6)\n",
        "opt2 = keras.optimizers.Adam(lr=0.0001)\n",
        "opt3 = keras.optimizers.RMSprop(learning_rate = 0.0001,decay =1e-6)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt3,metrics=['accuracy'])  \n",
        "\n",
        "\n",
        "# Création du dossier pour sauvegrader le model\n",
        "if os.path.exists(result_path) == False:\n",
        "    os.makedirs(result_path)\n",
        "\n",
        "\n",
        "keras_callback = [EarlyStopping(monitor='val_loss',patience = 5, verbose = 2)]\n",
        "\n",
        "history=model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch=math.ceil(len(train_ds)),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=math.ceil(len(val_ds)),\n",
        "    verbose=1,\n",
        "    callbacks = keras_callback\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVOg-VgI9YMB"
      },
      "source": [
        "#**14. Save your model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbAlAYpm9fg5"
      },
      "source": [
        "model.save('xception2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_ljTXlMSIZ"
      },
      "source": [
        "#**15. Visualization of training/validation curves**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIwdjee_KLk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\t'test_data/test',          # chemin vers le jeu de données\n",
        "\tseed=42,                    # Initialisation du générateur aléatoire (permutations)\n",
        "\timage_size=(input_dim,input_dim),       # Taille des images d'entrée\n",
        "\tbatch_size=nbr_batch_size,      # Taille du mini-batch\n",
        "  label_mode='categorical'    # Conversion au format One-Hot\n",
        ")"
      ],
      "metadata": {
        "id": "bvX9QXRCh1GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_ds,  steps=len(test_ds),workers = 1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "id": "LxHxT0GFiQK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oECfYKSp_MaQ"
      },
      "source": [
        "#**16. Test the model with a test image**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"https://www.ecologie.gouv.fr/sites/default/files/styles/standard/public/Feux.png\""
      ],
      "metadata": {
        "id": "hcKZUDoWUq0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_JRsQoh_chG"
      },
      "source": [
        "%matplotlib inline\n",
        "classes = train_ds.class_names\n",
        "image_path =  \"fog.jpg\"\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "x = tf.keras.utils.img_to_array(img,data_format='channels_last')\n",
        "x = tf.keras.preprocessing.image.smart_resize(x, size=(input_dim,input_dim))\n",
        "x = np.expand_dims(x, axis=0)\n",
        "# predict\n",
        "pred = model.predict(x,batch_size=1)[0]\n",
        "\n",
        "for (pos,prob) in enumerate(pred):\n",
        "    class_name = classes[pos]\n",
        "    if (pos == np.argmax(pred)) :\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        font = cv2.FONT_HERSHEY_COMPLEX \n",
        "        textsize = cv2.getTextSize(class_name, font, 1, 2)[0]\n",
        "        textX = (img.shape[1] - textsize[0]) / 2\n",
        "        textY = (img.shape[0] + textsize[1]) / 2\n",
        "        cv2.putText(img, class_name, (int(textX)-10, int(textY)), font, 2, (255,0,0), 6, cv2.LINE_AA)\n",
        "        plt.imshow(img)\n",
        "    print(\"Class Name : %s\" % (class_name), \"---\", \"Class Probability: %.2f%%\" % (prob*100))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}