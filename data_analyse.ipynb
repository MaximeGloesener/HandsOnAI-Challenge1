{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximeGloesener/HandsOnAI-Challenge1/blob/master/data_analyse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK6fZUXolwPs"
      },
      "source": [
        "# **1. Hardware Informations (GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjh8Rulquq"
      },
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash"
      ],
      "metadata": {
        "id": "uqK0POsY1lmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrtS3fRhb5x6"
      },
      "source": [
        "# **2. Importation of librairies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS3XuLGyb5x_"
      },
      "source": [
        "from IPython.display import Image, HTML, display\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np \n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input #224*224\n",
        "from keras.applications.xception import Xception, preprocess_input, decode_predictions #299*299\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions #224*224\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Flatten, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import math\n",
        "import argparse\n",
        "import matplotlib\n",
        "import imghdr\n",
        "import pickle as pkl\n",
        "import datetime\n",
        "from cycler import cycler\n",
        "from PIL import Image, ImageEnhance\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import imagehash\n",
        "print(\"Tensorflow version: \"+tf.__version__)\n",
        "print(\"Keras version: \" + tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6HqNeyYKraU"
      },
      "source": [
        "#**3. Download of training datasets \"FIRE_DATABASE_X\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CwAmMbqmGhW"
      },
      "source": [
        "bases_path_after=\"bases\"\n",
        "test=\"test_data\"\n",
        "if os.path.exists(bases_path_after) == False:\n",
        "    os.makedirs(bases_path_after)\n",
        "if not os.path.exists(test):\n",
        "  os.makedirs(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q archive\\ \\(1\\).zip"
      ],
      "metadata": {
        "id": "tT0FTdRI9okP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zdwkyRtmnN-"
      },
      "source": [
        "!rm -rf FIRE_DATABASE_1.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_1.tar\n",
        "!tar xf FIRE_DATABASE_1.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_1.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf FIRE_DATABASE_2.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_2.tar\n",
        "!tar xf FIRE_DATABASE_2.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_2.tar"
      ],
      "metadata": {
        "id": "e4aYRc5rluNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf FIRE_DATABASE_3.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_3.tar\n",
        "!tar xf FIRE_DATABASE_3.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_3.tar"
      ],
      "metadata": {
        "id": "WMfYa9uIluzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(\"/content/bases/FIRE_DATABASE_3/fire\"))+len(os.listdir(\"/content/bases/FIRE_DATABASE_3/no_fire\"))+len(os.listdir(\"/content/bases/FIRE_DATABASE_3/start_fire\"))"
      ],
      "metadata": {
        "id": "4pIfqYYIS32O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Données de test\n",
        "!rm -rf sample_data\n",
        "!wget --no-check-certificate http://195.154.53.219/downloads/test.tar\n",
        "! tar xf test.tar -C 'test_data' --one-top-level\n",
        "! rm test.tar"
      ],
      "metadata": {
        "id": "Us_JlUVUXamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important de tester les doublons en utilisant un hash cryptographique qui comparer les images pixels par pixels. Avec un hash robuste, on trouve des faux doublons. Le hash robuste permet de détecter les doublons si resize/légère modifiction mais ce n'est pas le cas ici dans les datasets. "
      ],
      "metadata": {
        "id": "prGdb9QW88gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(file_name):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée une path d'image et qui return RGB (utile pour plot)\n",
        "  \"\"\"\n",
        "  img = cv2.imread(file_name, 3)\n",
        "  b,g,r = cv2.split(img)\n",
        "  rgb_image = cv2.merge([r,g,b])\n",
        "  return rgb_image\n",
        "\n",
        "def plot(images, noms):\n",
        "  f, axarr = plt.subplots(1,len(images))\n",
        "  for i in range(len(images)):\n",
        "    axarr[i].imshow(images[i])\n",
        "    axarr[i].title.set_text(noms[i])\n"
      ],
      "metadata": {
        "id": "-T0WHM1aQShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des données\n",
        "# On sait que dans les datasets, il y a parfois plusieurs fois la même image\n",
        "# But : analyser chaque dataset et trouver le nombre d'images en doublons\n",
        "def analyse_dataset(folder_name, affichage = False):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée le directory d'un dataset et qui va chercher les images qui sont présentes plusieurs fois pour ce même dataset\n",
        "  Affichage = True si on veut plot les images qui sont en doubles et leur nom\n",
        "  Return: - le nombre de doublons dans un dataset\n",
        "          - le pourcentage de doublons\n",
        "  \"\"\"\n",
        "  img_hashes = dict()\n",
        "  total = 0\n",
        "  doublons = 0\n",
        "\n",
        "  for dir in os.listdir(folder_name):\n",
        "    for image in os.listdir(os.path.join(folder_name, dir)):\n",
        "      total += 1\n",
        "      image = os.path.join(os.getcwd(), folder_name, dir, image)\n",
        "      hash = imagehash.dhash(Image.open(image))\n",
        "      if hash in img_hashes:\n",
        "        doublons += 1\n",
        "        #print(f'{image} doublons de {img_hashes[hash]}')\n",
        "        if affichage:\n",
        "          i = read_image(image) \n",
        "          x = read_image(img_hashes[hash])\n",
        "          plot([x,i],[image.split(\"/\")[-1], img_hashes[hash].split(\"/\")[-1]])\n",
        "      else:\n",
        "        img_hashes[hash] = image\n",
        "\n",
        "  return doublons, doublons/total*100\n",
        "\n",
        "d1, p1 = analyse_dataset(\"/content/bases/FIRE_DATABASE_1/\")\n",
        "d2, p2 = analyse_dataset(\"/content/bases/FIRE_DATABASE_2/\")\n",
        "d3, p3 = analyse_dataset(\"/content/bases/FIRE_DATABASE_3/\", affichage = True)\n",
        "print('DATASET 1 ')\n",
        "print(f'Il y a {d1} doublons dans le dataset = {p1}% des données')\n",
        "print('DATASET 2 ')\n",
        "print(f'Il y a {d2} doublons dans le dataset = {p2}% des données')\n",
        "print('DATASET 3 ')\n",
        "print(f'Il y a {d3} doublons dans le dataset = {p3}% des données')\n"
      ],
      "metadata": {
        "id": "tIpQq5lygtsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un seul dataset à partir des 3 en ne prenant en compte que des images uniques (supprimer tous les doublons)\n",
        "def make_dataset(base_directory):\n",
        "  \"\"\"\n",
        "  Fonction qui va concaténer les 3 datasets de départ et créer un seul dataset sans doublons\n",
        "  Return les hashs des images déjà présentes dans le dataset -> utile lors de la phase data augmentation pour \n",
        "  ne pas rajouter des images qui sont déjà présentes dans la jeu de données \n",
        "  \"\"\"\n",
        "  !rm -rf all_data\n",
        "  directory = 'all_data'\n",
        "  directory_path = os.path.join(os.getcwd(), directory)\n",
        "  # créer un nouveau directory all_data s'il n'existe pas déjà\n",
        "  if not os.path.exists(directory_path):\n",
        "    os.mkdir(directory_path)\n",
        "    os.mkdir(os.path.join(directory_path, \"fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"no_fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"start_fire\"))\n",
        "\n",
        "  images_hash = set()\n",
        "  for dir in os.listdir(base_directory):\n",
        "    for dir2 in os.listdir(os.path.join(base_directory,dir)):\n",
        "      for img in os.listdir(os.path.join(base_directory,dir,dir2)):\n",
        "        path = os.path.join(os.getcwd(), base_directory, dir, dir2, img)\n",
        "        hash = imagehash.dhash(Image.open(path))\n",
        "        if hash not in images_hash:\n",
        "          images_hash.add(hash)\n",
        "          cv2.imwrite(os.path.join(directory_path, dir2, img), cv2.imread(path))\n",
        "  return images_hash \n",
        "hashes = make_dataset(\"bases\")"
      ],
      "metadata": {
        "id": "e0FpgawA8NHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stocker les hashs déjà présent dans notre DB d'images pour ne pas par la suite, rajouter des images déjà présentes\n",
        "with open('hashes.txt','w') as fout:\n",
        "  for hash in hashes:\n",
        "    fout.write(f\"{hash}\\n\")\n"
      ],
      "metadata": {
        "id": "sDBUbbfm8JuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_dataset(\"/content/all_data\", affichage=True)"
      ],
      "metadata": {
        "id": "v44IwS4k4hNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(\"/content/all_data/fire\"))+len(os.listdir(\"/content/all_data/no_fire\"))+len(os.listdir(\"/content/all_data/start_fire\"))"
      ],
      "metadata": {
        "id": "-lVorrb-CLT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des images dans un directory\n",
        "directory = 'bases/test/start_fire'\n",
        "\n",
        "for index, img in enumerate(os.listdir(directory)):\n",
        "  img = os.path.join(os.getcwd(), directory, img)\n",
        "  fig = plt.figure()\n",
        "  image = read_image(img)\n",
        "  plt.imshow(image)\n",
        "  plt.title(img)"
      ],
      "metadata": {
        "id": "-hc06Ha0bfZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connexion au drive et copier les données et les hash sur le drive et dans le folder partagé\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cp -av \"/content/all_data/\" \"/content/gdrive/MyDrive/Challenge1/\"\n",
        "%cp -av \"/content/hashes.txt\" \"/content/gdrive/MyDrive/Challenge1/\""
      ],
      "metadata": {
        "id": "lS51kMVkBiJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}