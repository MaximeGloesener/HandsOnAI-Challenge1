{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximeGloesener/HandsOnAI-Challenge1/blob/master/1-DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Importation des librairies**"
      ],
      "metadata": {
        "id": "Gu32sL46q82w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjh8Rulquq"
      },
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash"
      ],
      "metadata": {
        "id": "uqK0POsY1lmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS3XuLGyb5x_"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import imagehash\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6HqNeyYKraU"
      },
      "source": [
        "#**2. Téléchargement des données d'entraînement et de test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CwAmMbqmGhW"
      },
      "source": [
        "bases_path_after=\"bases\"\n",
        "test=\"test_data\"\n",
        "if not os.path.exists(bases_path_after):\n",
        "    os.makedirs(bases_path_after)\n",
        "if not os.path.exists(test):\n",
        "  os.makedirs(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zdwkyRtmnN-"
      },
      "source": [
        "!rm -rf FIRE_DATABASE_1.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_1.tar\n",
        "!tar xf FIRE_DATABASE_1.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_1.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf FIRE_DATABASE_2.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_2.tar\n",
        "!tar xf FIRE_DATABASE_2.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_2.tar"
      ],
      "metadata": {
        "id": "e4aYRc5rluNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf FIRE_DATABASE_3.tar\n",
        "!rm -rf sample_data\n",
        "!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_3.tar\n",
        "!tar xf FIRE_DATABASE_3.tar -C 'bases' --one-top-level\n",
        "!rm FIRE_DATABASE_3.tar"
      ],
      "metadata": {
        "id": "WMfYa9uIluzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Données de test\n",
        "!rm -rf sample_data\n",
        "!wget --no-check-certificate http://195.154.53.219/downloads/test.tar\n",
        "! tar xf test.tar -C 'test_data' --one-top-level\n",
        "! rm test.tar"
      ],
      "metadata": {
        "id": "Us_JlUVUXamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Analyse des données**"
      ],
      "metadata": {
        "id": "YU432TMirVt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(file_name):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée une path d'image et qui convertit l'image en RGB (utile pour plot avec matplotlib)\n",
        "  \"\"\"\n",
        "  img = cv2.imread(file_name, 3)\n",
        "  b,g,r = cv2.split(img)\n",
        "  rgb_image = cv2.merge([r,g,b])\n",
        "  return rgb_image"
      ],
      "metadata": {
        "id": "4NBNB2o9r5Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des images dans un directory: \n",
        "# -> permet d'identifier les problèmes dans le dataset (images aberrantes / présence de doublons)\n",
        "directory = 'bases/test/start_fire'\n",
        "\n",
        "for index, img in enumerate(os.listdir(directory)):\n",
        "  img = os.path.join(os.getcwd(), directory, img)\n",
        "  fig = plt.figure()\n",
        "  image = read_image(img)\n",
        "  plt.imshow(image)\n",
        "  plt.title(img)"
      ],
      "metadata": {
        "id": "FZ78692-sEb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Identification des doublons**"
      ],
      "metadata": {
        "id": "UGHyqqoas3kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(images, noms):\n",
        "  \"\"\"\n",
        "  Fonction utilisée pour afficher les doublons côte à côte \n",
        "  Permet de tester différent hash et de choisir la méthode de hachage avec le moins de faux positifs\n",
        "  \"\"\"\n",
        "  f, axarr = plt.subplots(1,len(images))\n",
        "  for i in range(len(images)):\n",
        "    axarr[i].imshow(images[i])\n",
        "    axarr[i].title.set_text(noms[i])\n"
      ],
      "metadata": {
        "id": "-T0WHM1aQShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On a pu identifier la présence de doublons dans les jeux de données\n",
        "# But : analyser chaque dataset et trouver le nombre d'images en doublons\n",
        "def analyse_dataset(folder_name, affichage = False):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée le directory d'un dataset et qui va chercher les images qui sont présentes plusieurs fois pour ce même dataset\n",
        "  Affichage = True si on veut plot les images qui sont en doubles et leur nom\n",
        "  Return: - le nombre de doublons dans un dataset\n",
        "          - le pourcentage de doublons\n",
        "  \"\"\"\n",
        "  img_hashes = dict()\n",
        "  total = 0\n",
        "  doublons = 0\n",
        "\n",
        "  for dir in os.listdir(folder_name):\n",
        "    for image in os.listdir(os.path.join(folder_name, dir)):\n",
        "      total += 1\n",
        "      image = os.path.join(os.getcwd(), folder_name, dir, image)\n",
        "      hash = imagehash.dhash(Image.open(image))\n",
        "      if hash in img_hashes:\n",
        "        doublons += 1\n",
        "        #print(f'{image} doublons de {img_hashes[hash]}')\n",
        "        if affichage:\n",
        "          i = read_image(image) \n",
        "          x = read_image(img_hashes[hash])\n",
        "          plot([x,i],[image.split(\"/\")[-1], img_hashes[hash].split(\"/\")[-1]])\n",
        "      else:\n",
        "        img_hashes[hash] = image\n",
        "\n",
        "  return doublons, doublons/total*100\n",
        "\n",
        "d1, p1 = analyse_dataset(\"/content/bases/FIRE_DATABASE_1/\")\n",
        "d2, p2 = analyse_dataset(\"/content/bases/FIRE_DATABASE_2/\")\n",
        "d3, p3 = analyse_dataset(\"/content/bases/FIRE_DATABASE_3/\", affichage = True)\n",
        "print('DATASET 1 ')\n",
        "print(f'Il y a {d1} doublons dans le dataset = {p1}% des données')\n",
        "print('DATASET 2 ')\n",
        "print(f'Il y a {d2} doublons dans le dataset = {p2}% des données')\n",
        "print('DATASET 3 ')\n",
        "print(f'Il y a {d3} doublons dans le dataset = {p3}% des données')\n"
      ],
      "metadata": {
        "id": "tIpQq5lygtsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Création d'un seul dataset sans doublon**"
      ],
      "metadata": {
        "id": "tBuOZ6b3tawY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer un seul dataset à partir des 3 en ne prenant en compte que des images uniques (supprimer tous les doublons)\n",
        "def make_dataset(base_directory):\n",
        "  \"\"\"\n",
        "  Fonction qui va concaténer les 3 datasets de départ et créer un seul dataset sans doublons\n",
        "  Return les hashs des images déjà présentes dans le dataset -> utile lors de la phase data augmentation pour \n",
        "  ne pas rajouter des images qui sont déjà présentes dans la jeu de données \n",
        "  \"\"\"\n",
        "  !rm -rf all_data\n",
        "  directory = 'all_data'\n",
        "  directory_path = os.path.join(os.getcwd(), directory)\n",
        "  # créer un nouveau directory all_data s'il n'existe pas déjà\n",
        "  if not os.path.exists(directory_path):\n",
        "    os.mkdir(directory_path)\n",
        "    os.mkdir(os.path.join(directory_path, \"fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"no_fire\"))\n",
        "    os.mkdir(os.path.join(directory_path, \"start_fire\"))\n",
        "\n",
        "  images_hash = set()\n",
        "  for dir in os.listdir(base_directory):\n",
        "    for dir2 in os.listdir(os.path.join(base_directory,dir)):\n",
        "      for img in os.listdir(os.path.join(base_directory,dir,dir2)):\n",
        "        path = os.path.join(os.getcwd(), base_directory, dir, dir2, img)\n",
        "        hash = imagehash.dhash(Image.open(path))\n",
        "        if hash not in images_hash:\n",
        "          images_hash.add(hash)\n",
        "          cv2.imwrite(os.path.join(directory_path, dir2, img), cv2.imread(path))\n",
        "  return images_hash \n",
        "hashes = make_dataset(\"bases\")"
      ],
      "metadata": {
        "id": "e0FpgawA8NHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Stockage des hashs**"
      ],
      "metadata": {
        "id": "36CJVZFOug5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stocker les hashs déjà présents dans notre DB d'images pour ne pas par la suite, rajouter des images déjà présentes\n",
        "with open('hashes.txt','w') as fout:\n",
        "  for hash in hashes:\n",
        "    fout.write(f\"{hash}\\n\")"
      ],
      "metadata": {
        "id": "sDBUbbfm8JuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Identification du nombre de données**"
      ],
      "metadata": {
        "id": "nfui6u2_uvQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fire = len(os.listdir(\"/content/all_data/fire\"))\n",
        "no_fire = len(os.listdir(\"/content/all_data/no_fire\"))\n",
        "start_fire = len(os.listdir(\"/content/all_data/start_fire\"))\n",
        "print(f\"Nombre d'images de la classe fire : {fire}\")\n",
        "print(f\"Nombre d'images de la classe no fire : {no_fire}\")\n",
        "print(f\"Nombre d'images de la classe start_fire: {start_fire}\")\n",
        "print(f\"Nombre total d'images: {fire+start_fire+no_fire}\")"
      ],
      "metadata": {
        "id": "-lVorrb-CLT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Stockage des données sur un drive**"
      ],
      "metadata": {
        "id": "OResFLnfwaVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connexion au drive et copier les données et les hash sur le drive et dans le folder partagé\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cp -av \"/content/all_data/\" \"/content/gdrive/MyDrive/Challenge1/\"\n",
        "%cp -av \"/content/hashes.txt\" \"/content/gdrive/MyDrive/Challenge1/\""
      ],
      "metadata": {
        "id": "lS51kMVkBiJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}