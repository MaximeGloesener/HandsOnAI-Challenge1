{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtv0IRZ8dZOeRGXbaRX37S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximeGloesener/HandsOnAI-Challenge1/blob/master/ajout_donn%C3%A9es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W81cDIJOizH"
      },
      "outputs": [],
      "source": [
        "# On veut récupérer les données qui ont été traitées dans le premier notebook et qui sont stockées sur le drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ImageHash"
      ],
      "metadata": {
        "id": "HIgFgxeaRHhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/gdrive/MyDrive/Challenge1/all_data/'"
      ],
      "metadata": {
        "id": "iwzBoO_rPCau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On récupère les hashs qu'on avait stocké lors de la première étape\n",
        "# Permet de connaître les images déjà présentes en DB et ne pas rajouter des images déjà présentes\n",
        "hashs = set()\n",
        "with open(\"/content/gdrive/MyDrive/Challenge1/hashes.txt\") as fin:\n",
        "  for hash in fin:\n",
        "    hashs.add(hash.strip())"
      ],
      "metadata": {
        "id": "jlApsC3_PTAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def read_image(img):\n",
        "  image = cv2.imread(img)\n",
        "  b,g,r = cv2.split(image)\n",
        "  rgb_img = cv2.merge([r,g,b])\n",
        "  return rgb_img"
      ],
      "metadata": {
        "id": "5vKSPL-5PlGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imagehash\n",
        "from PIL import Image\n",
        "import os \n",
        "\n",
        "def add_image(hashs, img, dataset_path, class_path):\n",
        "  \"\"\"\n",
        "  Fonction qui prend en entrée: \n",
        "  - hashs        = un set avec les hashs déjà présents en DB\n",
        "  - img          = nouvelle image qu'on veut ajouter au dataset\n",
        "  - dataset_path = le path du dataset\n",
        "  - class_path   = fire / no_fire / start_fire\n",
        "\n",
        "  \"\"\"\n",
        "  path = os.path.join(dataset_path, class_path)\n",
        "  if imagehash.dhash(Image.open(img)) not in hashs:\n",
        "    # add image dans la bonne classe \n",
        "    cv2.imwrite(img, path)\n",
        "    # ajouter le hash en db\n",
        "    # réfléchir au meilleur moyen de update les hashs => soit in met à jour la variable et à la fin et réécrit le fichier\n",
        "    # hashs sur le drive, soit on update le fichier hashs et on le lit à chaque fois, ou autre idée plus smart ?\n",
        "\n"
      ],
      "metadata": {
        "id": "TJRO8bE6RAOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}